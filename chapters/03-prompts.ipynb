{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wOYUHE2cr9nF"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/aurelio-labs/langchain-course/blob/main/chapters/03-prompts.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JuFGPQNZryUt"
      },
      "source": [
        "### LangChain Essentials"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ww7iP72ryUv"
      },
      "source": [
        "# Prompts Templating for OpenAI - LangChain"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bme97GW3ryUv"
      },
      "source": [
        "Until 2021, to use an AI model for a specific use-case we would need to fine-tune the model weights themselves. That would require huge amounts of training data and significant compute to fine-tune any reasonably performing model.\n",
        "\n",
        "Instruction fine-tuned **L**arge **L**anguage **M**odels (LLMs) changed this fundamental rule of applying AI models to new use-cases. Rather than needing to either train a model from scratch or fine-tune an existing model, these new LLMs could adapt incredibly well to a new problem or use-case with nothing more than a prompt change.\n",
        "\n",
        "Prompts allow us to completely change the functionality of an AI pipeline. Through natural language we simply _tell_ our LLM what it needs to do, and with the right AI pipeline and prompting, it often works.\n",
        "\n",
        "LangChain naturally has many functionalities geared towards helping us build our prompts. We can build very dynamic prompting pipelines that modifying the structure and content of what we feed into our LLM based on essentially any parameter we would like. In this example, we'll explore the essentials to prompting in LangChain and apply this in a demo **R**etrieval **A**ugmented **G**eneration (RAG) pipeline.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "E6aKPEfPsYNf",
        "outputId": "12a4e221-74cd-4039-d762-3759f6784455",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain==0.2.16\n",
            "  Downloading langchain-0.2.16-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting langchain-openai==0.1.7\n",
            "  Downloading langchain_openai-0.1.7-py3-none-any.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.12/dist-packages (from langchain==0.2.16) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.12/dist-packages (from langchain==0.2.16) (2.0.43)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.12/dist-packages (from langchain==0.2.16) (3.12.15)\n",
            "Collecting langchain-core<0.3.0,>=0.2.38 (from langchain==0.2.16)\n",
            "  Downloading langchain_core-0.2.43-py3-none-any.whl.metadata (6.2 kB)\n",
            "Collecting langchain-text-splitters<0.3.0,>=0.2.0 (from langchain==0.2.16)\n",
            "  Downloading langchain_text_splitters-0.2.4-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting langsmith<0.2.0,>=0.1.17 (from langchain==0.2.16)\n",
            "  Downloading langsmith-0.1.147-py3-none-any.whl.metadata (14 kB)\n",
            "Collecting numpy<2.0.0,>=1.26.0 (from langchain==0.2.16)\n",
            "  Downloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.12/dist-packages (from langchain==0.2.16) (2.11.9)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.12/dist-packages (from langchain==0.2.16) (2.32.4)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain==0.2.16) (8.5.0)\n",
            "Requirement already satisfied: openai<2.0.0,>=1.24.0 in /usr/local/lib/python3.12/dist-packages (from langchain-openai==0.1.7) (1.108.0)\n",
            "Requirement already satisfied: tiktoken<1,>=0.7 in /usr/local/lib/python3.12/dist-packages (from langchain-openai==0.1.7) (0.11.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.2.16) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.2.16) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.2.16) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.2.16) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.2.16) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.2.16) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.2.16) (1.20.1)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.12/dist-packages (from langchain-core<0.3.0,>=0.2.38->langchain==0.2.16) (1.33)\n",
            "Collecting packaging<25,>=23.2 (from langchain-core<0.3.0,>=0.2.38->langchain==0.2.16)\n",
            "  Downloading packaging-24.2-py3-none-any.whl.metadata (3.2 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.12/dist-packages (from langchain-core<0.3.0,>=0.2.38->langchain==0.2.16) (4.15.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain==0.2.16) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain==0.2.16) (3.11.3)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain==0.2.16) (1.0.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai<2.0.0,>=1.24.0->langchain-openai==0.1.7) (4.10.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai<2.0.0,>=1.24.0->langchain-openai==0.1.7) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from openai<2.0.0,>=1.24.0->langchain-openai==0.1.7) (0.11.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai<2.0.0,>=1.24.0->langchain-openai==0.1.7) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai<2.0.0,>=1.24.0->langchain-openai==0.1.7) (4.67.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1->langchain==0.2.16) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1->langchain==0.2.16) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1->langchain==0.2.16) (0.4.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain==0.2.16) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain==0.2.16) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain==0.2.16) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain==0.2.16) (2025.8.3)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from SQLAlchemy<3,>=1.4->langchain==0.2.16) (3.2.4)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken<1,>=0.7->langchain-openai==0.1.7) (2024.11.6)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain==0.2.16) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain==0.2.16) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.38->langchain==0.2.16) (3.0.0)\n",
            "Downloading langchain-0.2.16-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_openai-0.1.7-py3-none-any.whl (34 kB)\n",
            "Downloading langchain_core-0.2.43-py3-none-any.whl (397 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m397.1/397.1 kB\u001b[0m \u001b[31m28.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_text_splitters-0.2.4-py3-none-any.whl (25 kB)\n",
            "Downloading langsmith-0.1.147-py3-none-any.whl (311 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.8/311.8 kB\u001b[0m \u001b[31m22.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.0/18.0 MB\u001b[0m \u001b[31m90.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading packaging-24.2-py3-none-any.whl (65 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.5/65.5 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: packaging, numpy, langsmith, langchain-core, langchain-text-splitters, langchain-openai, langchain\n",
            "  Attempting uninstall: packaging\n",
            "    Found existing installation: packaging 25.0\n",
            "    Uninstalling packaging-25.0:\n",
            "      Successfully uninstalled packaging-25.0\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "  Attempting uninstall: langsmith\n",
            "    Found existing installation: langsmith 0.4.28\n",
            "    Uninstalling langsmith-0.4.28:\n",
            "      Successfully uninstalled langsmith-0.4.28\n",
            "  Attempting uninstall: langchain-core\n",
            "    Found existing installation: langchain-core 0.3.76\n",
            "    Uninstalling langchain-core-0.3.76:\n",
            "      Successfully uninstalled langchain-core-0.3.76\n",
            "  Attempting uninstall: langchain-text-splitters\n",
            "    Found existing installation: langchain-text-splitters 0.3.11\n",
            "    Uninstalling langchain-text-splitters-0.3.11:\n",
            "      Successfully uninstalled langchain-text-splitters-0.3.11\n",
            "  Attempting uninstall: langchain\n",
            "    Found existing installation: langchain 0.3.27\n",
            "    Uninstalling langchain-0.3.27:\n",
            "      Successfully uninstalled langchain-0.3.27\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed langchain-0.2.16 langchain-core-0.2.43 langchain-openai-0.1.7 langchain-text-splitters-0.2.4 langsmith-0.1.147 numpy-1.26.4 packaging-24.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "langchain",
                  "langchain_core",
                  "langsmith",
                  "numpy",
                  "packaging"
                ]
              },
              "id": "bc70a13a8fde427eb9d1206f976bbacf"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "# !pip install -qU \\\n",
        "#   langchain-core==0.3.33 \\\n",
        "#   langchain-openai==0.3.3 \\\n",
        "#   langchain-community==0.3.16 \\\n",
        "#   langsmith==0.3.4\n",
        "\n",
        "!pip install \"langchain==0.2.16\" \"langchain-openai==0.1.7\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F33PFuisryUv"
      },
      "source": [
        "---\n",
        "\n",
        "> ⚠️ We will be using OpenAI for this example allowing us to run everything via API. If you would like to use Ollama instead, check out the [Ollama LangChain Course](https://github.com/aurelio-labs/langchain-course/tree/main/notebooks/ollama).\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AaNrSZdlryUw"
      },
      "source": [
        "---\n",
        "\n",
        "> ⚠️ If using LangSmith, add your API key below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "GJAH_3KTryUw"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from getpass import getpass\n",
        "from google.colab import userdata\n",
        "\n",
        "os.environ[\"LANGCHAIN_API_KEY\"] = userdata.get(\"LANGCHAIN_API_KEY\") or \\\n",
        "    os.getenv(\"LANGCHAIN_API_KEY\") or \\\n",
        "    getpass(\"Enter LangSmith API Key: \")\n",
        "\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
        "os.environ[\"LANGCHAIN_PROJECT\"] = \"aurelioai-langchain-course-prompts-openai\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lAJf0RakryUw"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gb0lil1pryUw"
      },
      "source": [
        "## Basic Prompting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oZMTefWKryUw"
      },
      "source": [
        "We'll start by looking at the various parts of our prompt. For RAG use-cases we'll typically have three core components however this is _very_ use-cases dependant and can vary significantly. Nonetheless, for RAG we will typically see:\n",
        "\n",
        "* **Rules for our LLM**: this part of the prompt sets up the behavior of our LLM, how it should approach responding to user queries, and simply providing as much information as possible about what we're wanting to do as possible. We typically place this within the _system prompt_ of an chat LLM.\n",
        "\n",
        "* **Context**: this part is RAG-specific. The context refers to some _external information_ that we may have retrieved from a web search, database query, or often a _vector database_. This external information is the **R**etrieval **A**ugmentation part of **RA**G. For chat LLMs we'll typically place this inside the chat messages between the assistant and user.\n",
        "\n",
        "* **Question**: this is the input from our user. In the vast majority of cases the question/query/user input will always be provided to the LLM (and typically through a _user message_). However, the format and location of this being provided often changes.\n",
        "\n",
        "* **Answer**: this is the answer from our assistant, again this is _very_ typical and we'd expect this with every use-case.\n",
        "\n",
        "The below is an example of how a RAG prompt may look:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ll8Wq33DryUx"
      },
      "source": [
        "```\n",
        "Answer the question based on the context below,                 }\n",
        "if you cannot answer the question using the                     }--->  (Rules) For Our Prompt\n",
        "provided information answer with \"I don't know\"                 }\n",
        "\n",
        "Context: Aurelio AI is an AI development studio                 }\n",
        "focused on the fields of Natural Language Processing (NLP)      }\n",
        "and information retrieval using modern tooling                  }--->   Context AI has\n",
        "such as Large Language Models (LLMs),                           }\n",
        "vector databases, and LangChain.                                }\n",
        "\n",
        "Question: Does Aurelio AI do anything related to LangChain?     }--->   User Question\n",
        "\n",
        "Answer:                                                         }--->   AI Answer\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PLTVff_fryUx"
      },
      "source": [
        "Here we can see how the AI will appoach our question, as you can see we have a formulated response, if the context has the answer, then use the context to answer the question, if not, say I don't know, then we also have context and question which are being passed into this similarly to paramaters in a function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Acf6rPrhryUx"
      },
      "outputs": [],
      "source": [
        "prompt = \"\"\"\n",
        "Answer the user's query based on the context below.\n",
        "If you cannot answer the question using the\n",
        "provided information answer with \"I don't know\".\n",
        "\n",
        "Context: {context}\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rTc4UoqkryUx"
      },
      "source": [
        "LangChain uses a `ChatPromptTemplate` object to format the various prompt types into a single list which will be passed to our LLM:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "iC1F_FdKryUx"
      },
      "outputs": [],
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "# passing the template to the LangChain model\n",
        "prompt_template = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", prompt),\n",
        "    (\"user\", \"{query}\"),\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BXmZnfSqryUx"
      },
      "source": [
        "When we call the template it will expect us to provide two variables, the `context` and the `query`. Both of these variables are pulled from the strings we wrote, as LangChain interprets curly-bracket syntax (ie `{context}` and `{query}`) as indicating a dynamic variable that we expect to be inserted at query time. We can see that these variables have been picked up by our template object by viewing it's `input_variables` attribute:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zx4T96PVryUx",
        "outputId": "4748c0dc-edfe-4fdd-b411-6cd8476f6e33"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['context', 'query']"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "prompt_template.input_variables"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zXfKlpdoryUx"
      },
      "source": [
        "We can also view the structure of the messages (currently _prompt templates_) that the `ChatPromptTemplate` will construct by viewing the `messages` attribute:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HlgLvsV6ryUx",
        "outputId": "1ce23766-9e35-49dc-b4e5-417728693502"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template='\\nAnswer the user\\'s query based on the context below.\\nIf you cannot answer the question using the\\nprovided information answer with \"I don\\'t know\".\\n\\nContext: {context}\\n'), additional_kwargs={}),\n",
              " HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['query'], input_types={}, partial_variables={}, template='{query}'), additional_kwargs={})]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "prompt_template.messages"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BI8MHB4RryUx"
      },
      "source": [
        "From this, we can see that each tuple provided when using `ChatPromptTemplate.from_messages` becomes an individual prompt template itself. Within each of these tuples, the first value defines the _role_ of the message, which is typically `system`, `human`, or `ai`. Using these tuples is shorthand for the following, more explicit code:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "f37FWLOcryUy"
      },
      "outputs": [],
      "source": [
        "from langchain.prompts import (\n",
        "    SystemMessagePromptTemplate,\n",
        "    HumanMessagePromptTemplate\n",
        ")\n",
        "\n",
        "prompt_template = ChatPromptTemplate.from_messages([\n",
        "    SystemMessagePromptTemplate.from_template(prompt),\n",
        "    HumanMessagePromptTemplate.from_template(\"{query}\"),\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9uLUhXwWryUy"
      },
      "source": [
        "We can see the structure of this new chat prompt template is identical to our previous:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2z3KfV5JryUy",
        "outputId": "80d5860b-9b43-4a0f-a617-9fd5b308dffb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template='\\nAnswer the user\\'s query based on the context below.\\nIf you cannot answer the question using the\\nprovided information answer with \"I don\\'t know\".\\n\\nContext: {context}\\n'), additional_kwargs={}),\n",
              " HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['query'], input_types={}, partial_variables={}, template='{query}'), additional_kwargs={})]"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "prompt_template.messages"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yL63ZVTSryUy"
      },
      "source": [
        "### Invoking our LLM with Templates"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2RqGAcPzryUy"
      },
      "source": [
        "We've defined our prompt template, now let's define out LLM and run it with our template and a user query.\n",
        "\n",
        "We start by initializing OpenAI's `gpt-4o-mini`. If you need an API key you can get one from [OpenAI's website](https://platform.openai.com/settings/organization/api-keys)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "pHn3m62kryUy"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from getpass import getpass\n",
        "from google.colab import userdata\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get(\"OPENAI_API_KEY\") or \\\n",
        "    os.getenv(\"OPENAI_API_KEY\") or \\\n",
        "    getpass(\"Enter OpenAI API Key: \")\n",
        "\n",
        "openai_model = \"gpt-4o-mini\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ldJJqsMrryUy"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "llm = ChatOpenAI(temperature=0.0, model=openai_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jn0cn8ebryUy"
      },
      "source": [
        "Here we define our LLM and _because_ we're using it for a question-answer use-case we want it's answer to be as grounded in reality as possible. To do that, we ofcourse prompt it to not make up any information via the `If you cannot answer the question using the provided information answer with \"I don't know\"` line, but we _also_ use the model's `temperature` setting.\n",
        "\n",
        "The `temperature` parameter controls the randomness of the LLM's output. A temperature of `0.0` makes an LLM's output more determinstic which _in theory_ should lead to a lower likelihood of hallucination.\n",
        "\n",
        "Now, the question here may be, _why would we ever not use `temperature=0.0`?_ The answer to that is that sometimes a little bit of randomness can useful. Randomness tends to translate to text that feels more human and creative, so if we'd like an LLM to help us write an article or even a poem, that lack of determinism becomes a feature rather than a bug.\n",
        "\n",
        "For now, we'll stick with our more deterministic LLM. We'll setup the pipeline to consume two variables when our LLM pipeline is called, `query` and `context`, we'll feed them into our chat prompt template, and then invoke our LLM with our formatted messages.\n",
        "\n",
        "Although that sounds complicated, all we're doing is connecting our `prompt_template` and `llm`. We do this with **L**ang**C**hain **E**xpression **L**anguage (LCEL), which uses the `|` operator to connect our each component."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "_KhquCevryUy"
      },
      "outputs": [],
      "source": [
        "pipeline = (\n",
        "    {\n",
        "        \"query\": lambda x: x[\"query\"],\n",
        "        \"context\": lambda x: x[\"context\"]\n",
        "    }\n",
        "    | prompt_template\n",
        "    | llm\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SKSPk7x_ryUy"
      },
      "source": [
        "Now let's define a `query` and some relevant `context` and invoke our pipeline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "jhg8kQkbryUy"
      },
      "outputs": [],
      "source": [
        "context = \"\"\"Aurelio AI is an AI company developing tooling for AI\n",
        "engineers. Their focus is on language AI with the team having strong\n",
        "expertise in building AI agents and a strong background in\n",
        "information retrieval.\n",
        "\n",
        "The company is behind several open source frameworks, most notably\n",
        "Semantic Router and Semantic Chunkers. They also have an AI\n",
        "Platform providing engineers with tooling to help them build with\n",
        "AI. Finally, the team also provides development services to other\n",
        "organizations to help them bring their AI tech to market.\n",
        "\n",
        "Aurelio AI became LangChain Experts in September 2024 after a long\n",
        "track record of delivering AI solutions built with the LangChain\n",
        "ecosystem.\"\"\"\n",
        "\n",
        "query = \"what does Aurelio AI do?\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4pfbZIXIryUy",
        "outputId": "ae7f0e84-e48e-42cc-a0cd-1ea16d0c75de"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AIMessage(content='Aurelio AI is an AI company that develops tooling for AI engineers, focusing on language AI. They have expertise in building AI agents and information retrieval. The company is known for several open source frameworks, including Semantic Router and Semantic Chunkers, and offers an AI Platform that provides engineers with tools to build with AI. Additionally, they provide development services to help other organizations bring their AI technology to market.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 82, 'prompt_tokens': 183, 'total_tokens': 265, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_72ed7ab54c', 'finish_reason': 'stop', 'logprobs': None}, id='run-9559b71e-2a48-48f6-a258-4cdc5d16b91e-0', usage_metadata={'input_tokens': 183, 'output_tokens': 82, 'total_tokens': 265, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pipeline.invoke({\"query\": query, \"context\": context})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yNuW9yxDryUy"
      },
      "source": [
        "Our LLM pipeline is able to consume the information from the `context` and use it to answer the user's `query`. Ofcourse, we would not usually be feeding in both a question and an answer into an LLM manually. Typically, the `context` would be retrieved from a vector database, via web search, or from elsewhere. We will cover this use-case in full and build a functional RAG pipeline in a future chapter."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5zvOUzIlryUz"
      },
      "source": [
        "## Few Shot Prompting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EYa1xmwAryUz"
      },
      "source": [
        "Many **S**tate-**o**f-**t**he-**A**rt (SotA) LLMs are incredible at instruction following. Meaning that it requires much less effort to get the intended output or behavior from these models than is the case for older LLMs and smaller LLMs.\n",
        "\n",
        "Before creating an example let's first see how to use LangChain's few shot prompting objects. We will provide multiple examples and we'll feed them in as sequential human and ai messages so we setup the template like this:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "# example_prompt = ChatPromptTemplate.from_messages([\n",
        "#     (\"human\", \"{input}\"),\n",
        "#     (\"ai\", \"{output}\"),\n",
        "# ])\n",
        "\n",
        "# examples = [\n",
        "#     {\"input\": \"Hello\", \"output\": \"Xin chào\"},\n",
        "#     {\"input\": \"How are you?\", \"output\": \"Bạn khỏe không?\"},\n",
        "#     {\"input\": \"Good morning\", \"output\": \"Chào buổi sáng\"},\n",
        "# ]\n",
        "\n",
        "# from langchain.prompts import FewShotChatMessagePromptTemplate\n",
        "\n",
        "# few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
        "#     example_prompt=example_prompt,\n",
        "#     examples=examples,\n",
        "# )\n",
        "\n",
        "# final_prompt = ChatPromptTemplate.from_messages([\n",
        "#     (\"system\", \"You are a helpful translator from English to Vietnamese.\"),\n",
        "#     few_shot_prompt,   # <== chèn các ví dụ mẫu vào đây\n",
        "#     (\"human\", \"{query}\")  # <== câu hỏi mới\n",
        "# ])\n",
        "\n",
        "# print(final_prompt.format(query=\"I love programming.\"))\n"
      ],
      "metadata": {
        "id": "b1dAaQeUSsbh",
        "outputId": "1c254073-8079-4520-f7b4-bd147febe872",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "System: You are a helpful translator from English to Vietnamese.\n",
            "Human: Hello\n",
            "AI: Xin chào\n",
            "Human: How are you?\n",
            "AI: Bạn khỏe không?\n",
            "Human: Good morning\n",
            "AI: Chào buổi sáng\n",
            "Human: I love programming.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "iqTvW0BMryUz"
      },
      "outputs": [],
      "source": [
        "example_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"human\", \"{input}\"),\n",
        "    (\"ai\", \"{output}\"),\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3B6gYNmqryUz"
      },
      "source": [
        "Then we define a list of examples with dictionaries containing the correct `input` and `output` keys."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "iO-OT9-jryUz"
      },
      "outputs": [],
      "source": [
        "examples = [\n",
        "    {\"input\": \"Here is query #1\", \"output\": \"Here is the answer #1\"},\n",
        "    {\"input\": \"Here is query #2\", \"output\": \"Here is the answer #2\"},\n",
        "    {\"input\": \"Here is query #3\", \"output\": \"Here is the answer #3\"},\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QhaDY0vZryUz"
      },
      "source": [
        "We then feed both of these into our `FewShotChatMessagePromptTemplate` object:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hXTHA5T2ryUz",
        "outputId": "782a7df1-836b-4b0a-fa68-a5bb300cd918"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Human: Here is query #1\n",
            "AI: Here is the answer #1\n",
            "Human: Here is query #2\n",
            "AI: Here is the answer #2\n",
            "Human: Here is query #3\n",
            "AI: Here is the answer #3\n"
          ]
        }
      ],
      "source": [
        "from langchain.prompts import FewShotChatMessagePromptTemplate\n",
        "\n",
        "few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
        "    example_prompt=example_prompt,\n",
        "    examples=examples,\n",
        ")\n",
        "# here is the formatted prompt\n",
        "print(few_shot_prompt.format())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AsVWIVKLryU0"
      },
      "source": [
        "Using this we can provide different sets of `examples` or even different individual `example_prompt` templates to the `FewShotChatMessagePromptTemplate` object to build our prompt structure. Let's try an real example where we might use few-shot prompting.\n",
        "\n",
        "### Few-Shot Example"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nSN4tYI7ryU0"
      },
      "source": [
        "Using a tiny LLM limits it's ability, so when asking for specific behaviors or structured outputs it can struggle. For example, we'll ask the LLM to summarize the key points about Aurelio AI using markdown and bullet points. Let's see what happens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H_OOSKg-ryU0",
        "outputId": "81d471e6-8194-492c-9766-dbbf031ff856"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# Overview of Aurelio AI\n",
            "\n",
            "Aurelio AI is an AI company that specializes in developing tools and solutions for AI engineers, particularly in the realm of language AI. \n",
            "\n",
            "## Key Focus Areas\n",
            "- **Language AI Development**: Expertise in building AI agents and information retrieval systems.\n",
            "- **Open Source Frameworks**: Creator of notable frameworks like Semantic Router and Semantic Chunkers.\n",
            "- **AI Platform**: Provides engineers with tools to facilitate AI development.\n",
            "- **Development Services**: Offers services to help organizations bring their AI technologies to market.\n",
            "\n",
            "## Achievements\n",
            "- Became LangChain Experts in September 2024, showcasing their proficiency in the LangChain ecosystem.\n",
            "\n",
            "In conclusion, Aurelio AI is dedicated to empowering AI engineers through innovative tools, frameworks, and expert services in the field of language AI.\n"
          ]
        }
      ],
      "source": [
        "new_system_prompt = \"\"\"\n",
        "Answer the user's query based on the context below.\n",
        "If you cannot answer the question using the\n",
        "provided information answer with \"I don't know\".\n",
        "\n",
        "Always answer in markdown format. When doing so please\n",
        "provide headers, short summaries, follow with bullet\n",
        "points, then conclude.\n",
        "\n",
        "Context: {context}\n",
        "\"\"\"\n",
        "\n",
        "prompt_template.messages[0].prompt.template = new_system_prompt\n",
        "\n",
        "out = pipeline.invoke({\"query\": query, \"context\": context}).content\n",
        "print(out)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n-IUcKKVryU0"
      },
      "source": [
        "We can display our markdown nicely with `IPython` like so:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 309
        },
        "id": "yA5sXUejryU0",
        "outputId": "dcd6e8b6-0d84-42fc-8a07-ca2278049099"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# Overview of Aurelio AI\n\nAurelio AI is an AI company that specializes in developing tools and solutions for AI engineers, particularly in the realm of language AI. \n\n## Key Focus Areas\n- **Language AI Development**: Expertise in building AI agents and information retrieval systems.\n- **Open Source Frameworks**: Creator of notable frameworks like Semantic Router and Semantic Chunkers.\n- **AI Platform**: Provides engineers with tools to facilitate AI development.\n- **Development Services**: Offers services to help organizations bring their AI technologies to market.\n\n## Achievements\n- Became LangChain Experts in September 2024, showcasing their proficiency in the LangChain ecosystem.\n\nIn conclusion, Aurelio AI is dedicated to empowering AI engineers through innovative tools, frameworks, and expert services in the field of language AI."
          },
          "metadata": {}
        }
      ],
      "source": [
        "from IPython.display import display, Markdown\n",
        "\n",
        "display(Markdown(out))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2-st-PlBryU0"
      },
      "source": [
        "This is not bad, but also not quite the format we wanted. We could try improving our initial prompting instructions, but when this doesn't work we can move on to our few-shot prompting. We want to build something like this:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s6CCb3bIryU0"
      },
      "source": [
        "```\n",
        "Answer the user's query based on the context below,                 }\n",
        "if you cannot answer the question using the                         }\n",
        "provided information answer with \"I don't know\"                     }\n",
        "                                                                    }--->  (Rules)\n",
        "Always answer in markdown format. When doing so please              }\n",
        "provide headers, short summaries, follow with bullet                }\n",
        "points, then conclude. Here are some examples:                      }\n",
        "\n",
        "\n",
        "User: Can you explain gravity?                                      }\n",
        "AI: ## Gravity                                                      }\n",
        "                                                                    }\n",
        "Gravity is one of the fundamental forces in the universe.           }\n",
        "                                                                    }\n",
        "### Discovery                                                       }--->  (Example 1)\n",
        "                                                                    }\n",
        "* Gravity was first discovered by...                                }\n",
        "                                                                    }\n",
        "**To conclude**, Gravity is a fascinating topic and has been...     }\n",
        "                                                                    }\n",
        "\n",
        "User: What is the capital of France?                                }\n",
        "AI: ## France                                                       }\n",
        "                                                                    }\n",
        "The capital of France is Paris.                                     }\n",
        "                                                                    }--->  (Example 2)\n",
        "### Origins                                                         }\n",
        "                                                                    }\n",
        "* The name Paris comes from the...                                  }\n",
        "                                                                    }\n",
        "**To conclude**, Paris is highly regarded as one of the...          }\n",
        "\n",
        "Context: {context}                                                  }--->  (Context)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BbZYdVB9ryU0"
      },
      "source": [
        "We have already defined our `example_prompt` so now we just change our `examples` to use some examples of a user asking a question and the LLM answering in the exact markdown format we need."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "9CRNB9UDryU0"
      },
      "outputs": [],
      "source": [
        "examples = [\n",
        "    {\n",
        "        \"input\": \"Can you explain gravity?\",\n",
        "        \"output\": (\n",
        "            \"## Gravity\\n\\n\"\n",
        "            \"Gravity is one of the fundamental forces in the universe.\\n\\n\"\n",
        "            \"### Discovery\\n\\n\"\n",
        "            \"* Gravity was first discovered by Sir Isaac Newton in the late 17th century.\\n\"\n",
        "            \"* It was said that Newton theorized about gravity after seeing an apple fall from a tree.\\n\\n\"\n",
        "            \"### In General Relativity\\n\\n\"\n",
        "            \"* Gravity is described as the curvature of spacetime.\\n\"\n",
        "            \"* The more massive an object is, the more it curves spacetime.\\n\"\n",
        "            \"* This curvature is what causes objects to fall towards each other.\\n\\n\"\n",
        "            \"### Gravitons\\n\\n\"\n",
        "            \"* Gravitons are hypothetical particles that mediate the force of gravity.\\n\"\n",
        "            \"* They have not yet been detected.\\n\\n\"\n",
        "            \"**To conclude**, Gravity is a fascinating topic and has been studied extensively since the time of Newton.\\n\\n\"\n",
        "        )\n",
        "    },\n",
        "    {\n",
        "        \"input\": \"What is the capital of France?\",\n",
        "        \"output\": (\n",
        "            \"## France\\n\\n\"\n",
        "            \"The capital of France is Paris.\\n\\n\"\n",
        "            \"### Origins\\n\\n\"\n",
        "            \"* The name Paris comes from the Latin word \\\"Parisini\\\" which referred to a Celtic people living in the area.\\n\"\n",
        "            \"* The Romans named the city Lutetia, which means \\\"the place where the river turns\\\".\\n\"\n",
        "            \"* The city was renamed Paris in the 3rd century BC by the Celtic-speaking Parisii tribe.\\n\\n\"\n",
        "            \"**To conclude**, Paris is highly regarded as one of the most beautiful cities in the world and is one of the world's greatest cultural and economic centres.\\n\\n\"\n",
        "        )\n",
        "    }\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aPJgXmiCryU0"
      },
      "source": [
        "We feed these into our `FewShotChatMessagePromptTemplate` object:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "EatYep9JryU1"
      },
      "outputs": [],
      "source": [
        "few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
        "    example_prompt=example_prompt,\n",
        "    examples=examples,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Zf6BdXXryU1"
      },
      "source": [
        "Our formatted prompt now looks like this:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 507
        },
        "id": "imLL8987ryU1",
        "outputId": "3fedd051-184c-4358-d3b6-b4f810240283"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Human: Can you explain gravity?\nAI: ## Gravity\n\nGravity is one of the fundamental forces in the universe.\n\n### Discovery\n\n* Gravity was first discovered by Sir Isaac Newton in the late 17th century.\n* It was said that Newton theorized about gravity after seeing an apple fall from a tree.\n\n### In General Relativity\n\n* Gravity is described as the curvature of spacetime.\n* The more massive an object is, the more it curves spacetime.\n* This curvature is what causes objects to fall towards each other.\n\n### Gravitons\n\n* Gravitons are hypothetical particles that mediate the force of gravity.\n* They have not yet been detected.\n\n**To conclude**, Gravity is a fascinating topic and has been studied extensively since the time of Newton.\n\n\nHuman: What is the capital of France?\nAI: ## France\n\nThe capital of France is Paris.\n\n### Origins\n\n* The name Paris comes from the Latin word \"Parisini\" which referred to a Celtic people living in the area.\n* The Romans named the city Lutetia, which means \"the place where the river turns\".\n* The city was renamed Paris in the 3rd century BC by the Celtic-speaking Parisii tribe.\n\n**To conclude**, Paris is highly regarded as one of the most beautiful cities in the world and is one of the world's greatest cultural and economic centres.\n\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "out = few_shot_prompt.format()\n",
        "\n",
        "display(Markdown(out))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LpXGxDZtryU1"
      },
      "source": [
        "We then pull all of this together with our system prompt and final user query to create our final prompt and feed it into our LLM."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OLZWvcRS7Ez7",
        "outputId": "555b3585-38c8-48b4-c7a1-75b4cea862bc"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "FewShotChatMessagePromptTemplate(examples=[{'input': 'Can you explain gravity?', 'output': '## Gravity\\n\\nGravity is one of the fundamental forces in the universe.\\n\\n### Discovery\\n\\n* Gravity was first discovered by Sir Isaac Newton in the late 17th century.\\n* It was said that Newton theorized about gravity after seeing an apple fall from a tree.\\n\\n### In General Relativity\\n\\n* Gravity is described as the curvature of spacetime.\\n* The more massive an object is, the more it curves spacetime.\\n* This curvature is what causes objects to fall towards each other.\\n\\n### Gravitons\\n\\n* Gravitons are hypothetical particles that mediate the force of gravity.\\n* They have not yet been detected.\\n\\n**To conclude**, Gravity is a fascinating topic and has been studied extensively since the time of Newton.\\n\\n'}, {'input': 'What is the capital of France?', 'output': '## France\\n\\nThe capital of France is Paris.\\n\\n### Origins\\n\\n* The name Paris comes from the Latin word \"Parisini\" which referred to a Celtic people living in the area.\\n* The Romans named the city Lutetia, which means \"the place where the river turns\".\\n* The city was renamed Paris in the 3rd century BC by the Celtic-speaking Parisii tribe.\\n\\n**To conclude**, Paris is highly regarded as one of the most beautiful cities in the world and is one of the world\\'s greatest cultural and economic centres.\\n\\n'}], example_prompt=ChatPromptTemplate(input_variables=['input', 'output'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template='{input}')), AIMessagePromptTemplate(prompt=PromptTemplate(input_variables=['output'], template='{output}'))]))"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "few_shot_prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "cDwfI95_ryU1"
      },
      "outputs": [],
      "source": [
        "prompt_template = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", new_system_prompt),\n",
        "    few_shot_prompt,\n",
        "    (\"user\", \"{query}\"),\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZvEKhq2rryU1"
      },
      "source": [
        "Now feed this back into our pipeline:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 409
        },
        "id": "BJLEX-peryU1",
        "outputId": "87905432-4094-4b45-93ff-17aeb01ecd69"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "## Aurelio AI Overview\n\nAurelio AI is an AI company focused on developing tools and services for AI engineers, particularly in the realm of language AI.\n\n### Key Areas of Focus\n\n- **Language AI**: Specializes in creating solutions that enhance language processing capabilities.\n- **Open Source Frameworks**: Developed notable frameworks such as:\n  - **Semantic Router**\n  - **Semantic Chunkers**\n- **AI Platform**: Provides engineers with tools to facilitate AI development.\n- **Development Services**: Offers assistance to organizations in bringing their AI technologies to market.\n\n### Expertise\n\n- The team has strong expertise in building AI agents.\n- They possess a solid background in information retrieval.\n\n### Recognition\n\n- Became LangChain Experts in September 2024, showcasing their proficiency in the LangChain ecosystem.\n\n**To conclude**, Aurelio AI is dedicated to empowering AI engineers through innovative tools, frameworks, and development services in the language AI domain."
          },
          "metadata": {}
        }
      ],
      "source": [
        "pipeline = prompt_template | llm\n",
        "out = pipeline.invoke({\"query\": query, \"context\": context}).content\n",
        "display(Markdown(out))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UrEybF2dryU1"
      },
      "source": [
        "We can see that by adding a few examples to our prompt, ie _few-shot prompting_, we can get much more control over the exact structure of our LLM response. As the size of our LLMs increases, the ability of them to follow instructions becomes much greater and they tend to require less explicit prompting as we have shown here. However, even for SotA models like `gpt-4o` few-shot prompting is still a valid technique that can be used if the LLM is struggling to follow our intended instructions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SK1PU1dIryU1"
      },
      "source": [
        "## Chain of Thought Prompting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7CoiIYxmryU1"
      },
      "source": [
        "We'll take a look at one more commonly used prompting technique called _chain of thought_ (CoT). CoT is a technique that encourages the LLM to think through the problem step by step before providing an answer. The idea being that by breaking down the problem into smaller steps, the LLM is more likely to arrive at the correct answer and we are less likely to see hallucinations.\n",
        "\n",
        "To implement CoT we don't need any specific LangChain objects, instead we are simply modifying how we instruct our LLM within the system prompt. We will ask the LLM to list the problems that need to be solved, to solve each problem individually, and then to arrive at the final answer.\n",
        "\n",
        "Let's first test our LLM _without_ CoT prompting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ReF2-IUjryU1"
      },
      "outputs": [],
      "source": [
        "no_cot_system_prompt = \"\"\"\n",
        "Be a helpful assistant and answer the user's question.\n",
        "\n",
        "You MUST answer the question directly without any other\n",
        "text or explanation.\n",
        "\"\"\"\n",
        "\n",
        "no_cot_prompt_template = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", no_cot_system_prompt),\n",
        "    (\"user\", \"{query}\"),\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UBdDEZiTryU1"
      },
      "source": [
        "Nowadays most LLMs are trained to use CoT prompting by default, so we actually need to instruct it not to do so for this example which is why we added `\"You MUST answer the question directly without any other text or explanation.\"` to our system prompt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KBe_3bXaryU1",
        "outputId": "2938235c-ff26-4070-e832-0c2813495cc9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The total number of keystrokes needed to type the numbers from 1 to 500 is 1,511.\n"
          ]
        }
      ],
      "source": [
        "query = (\n",
        "    \"How many keystrokes are needed to type the numbers from 1 to 500?\"\n",
        ")\n",
        "\n",
        "no_cot_pipeline = no_cot_prompt_template | llm\n",
        "no_cot_result = no_cot_pipeline.invoke({\"query\": query}).content\n",
        "print(no_cot_result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vEy5iGS4ryU2"
      },
      "source": [
        "The actual answer is `1392`, but the LLM _without_ CoT just hallucinates and gives us a guess. Now, we can add explicit CoT prompting to our system prompt to see if we can get a better result."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ZYaNGO5ryU2"
      },
      "outputs": [],
      "source": [
        "# Define the chain-of-thought prompt template\n",
        "cot_system_prompt = \"\"\"\n",
        "Be a helpful assistant and answer the user's question.\n",
        "\n",
        "To answer the question, you must:\n",
        "\n",
        "- List systematically and in precise detail all\n",
        "  subproblems that need to be solved to answer the\n",
        "  question.\n",
        "- Solve each sub problem INDIVIDUALLY and in sequence.\n",
        "- Finally, use everything you have worked through to\n",
        "  provide the final answer.\n",
        "\"\"\"\n",
        "\n",
        "cot_prompt_template = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", cot_system_prompt),\n",
        "    (\"user\", \"{query}\"),\n",
        "])\n",
        "\n",
        "cot_pipeline = cot_prompt_template | llm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 875
        },
        "id": "Ke2vWBrOryU2",
        "outputId": "1db7ac2a-b9e0-4c57-b419-f8c4efbf4c92"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "To determine how many keystrokes are needed to type the numbers from 1 to 500, we can break this problem down into several subproblems:\n",
              "\n",
              "### Subproblems:\n",
              "1. Count the number of digits in the numbers from 1 to 9.\n",
              "2. Count the number of digits in the numbers from 10 to 99.\n",
              "3. Count the number of digits in the numbers from 100 to 499.\n",
              "4. Count the digits in the number 500.\n",
              "5. Sum all the digits counted in the previous steps to get the total number of keystrokes.\n",
              "\n",
              "### Step 1: Count the number of digits in the numbers from 1 to 9\n",
              "- The numbers from 1 to 9 are single-digit numbers.\n",
              "- There are 9 numbers (1, 2, 3, 4, 5, 6, 7, 8, 9).\n",
              "- Each number has 1 digit.\n",
              "\n",
              "**Total digits from 1 to 9 = 9 numbers × 1 digit = 9 digits.**\n",
              "\n",
              "### Step 2: Count the number of digits in the numbers from 10 to 99\n",
              "- The numbers from 10 to 99 are two-digit numbers.\n",
              "- There are 90 numbers (10, 11, 12, ..., 99).\n",
              "- Each number has 2 digits.\n",
              "\n",
              "**Total digits from 10 to 99 = 90 numbers × 2 digits = 180 digits.**\n",
              "\n",
              "### Step 3: Count the number of digits in the numbers from 100 to 499\n",
              "- The numbers from 100 to 499 are three-digit numbers.\n",
              "- There are 400 numbers (100, 101, 102, ..., 499).\n",
              "- Each number has 3 digits.\n",
              "\n",
              "**Total digits from 100 to 499 = 400 numbers × 3 digits = 1200 digits.**\n",
              "\n",
              "### Step 4: Count the digits in the number 500\n",
              "- The number 500 is a three-digit number.\n",
              "- It has 3 digits.\n",
              "\n",
              "**Total digits for 500 = 3 digits.**\n",
              "\n",
              "### Step 5: Sum all the digits counted\n",
              "Now we will sum the total digits from each of the previous steps:\n",
              "- From 1 to 9: 9 digits\n",
              "- From 10 to 99: 180 digits\n",
              "- From 100 to 499: 1200 digits\n",
              "- For 500: 3 digits\n",
              "\n",
              "**Total keystrokes = 9 + 180 + 1200 + 3 = 1392.**\n",
              "\n",
              "### Final Answer:\n",
              "The total number of keystrokes needed to type the numbers from 1 to 500 is **1392 keystrokes.**"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "cot_result = cot_pipeline.invoke({\"query\": query}).content\n",
        "display(Markdown(cot_result))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fw2o5yVEryU2"
      },
      "source": [
        "Now we get a much better result! Our LLM provides us with a final answer of `1392` which is correct. Finally, as mentioned most LLMs are now trained to use CoT prompting by default. So let's see what happens if we don't explicitly tell the LLM to use CoT."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LhpHDEUkryU2"
      },
      "outputs": [],
      "source": [
        "system_prompt = \"\"\"\n",
        "Be a helpful assistant and answer the user's question.\n",
        "\"\"\"\n",
        "\n",
        "prompt_template = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", system_prompt),\n",
        "    (\"user\", \"{query}\"),\n",
        "])\n",
        "\n",
        "pipeline = prompt_template | llm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 551
        },
        "id": "w102It7kryU2",
        "outputId": "d58caad0-097b-4d23-e7c4-0288cee7a79f"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "To calculate the total number of keystrokes needed to type the numbers from 1 to 500, we can break it down by the number of digits in the numbers.\n",
              "\n",
              "1. **Numbers from 1 to 9**: \n",
              "   - There are 9 numbers (1 to 9).\n",
              "   - Each number has 1 digit.\n",
              "   - Total keystrokes = 9 * 1 = 9.\n",
              "\n",
              "2. **Numbers from 10 to 99**: \n",
              "   - There are 90 numbers (10 to 99).\n",
              "   - Each number has 2 digits.\n",
              "   - Total keystrokes = 90 * 2 = 180.\n",
              "\n",
              "3. **Numbers from 100 to 499**: \n",
              "   - There are 400 numbers (100 to 499).\n",
              "   - Each number has 3 digits.\n",
              "   - Total keystrokes = 400 * 3 = 1200.\n",
              "\n",
              "4. **Number 500**: \n",
              "   - There is 1 number (500).\n",
              "   - It has 3 digits.\n",
              "   - Total keystrokes = 1 * 3 = 3.\n",
              "\n",
              "Now, we can sum all the keystrokes:\n",
              "\n",
              "- From 1 to 9: 9 keystrokes\n",
              "- From 10 to 99: 180 keystrokes\n",
              "- From 100 to 499: 1200 keystrokes\n",
              "- From 500: 3 keystrokes\n",
              "\n",
              "Total keystrokes = 9 + 180 + 1200 + 3 = 1392.\n",
              "\n",
              "Therefore, the total number of keystrokes needed to type the numbers from 1 to 500 is **1392**."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "result = pipeline.invoke({\"query\": query}).content\n",
        "display(Markdown(result))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4InfNSs_ryU2"
      },
      "source": [
        "We almost get the _exact_ same result. The formatting isn't quite as nice but the CoT behavior is clearly there, and the LLM produces the correct final answer!\n",
        "\n",
        "CoT is useful not only for simple question-answering like this, but is also a fundamental component of many agentic systems which will often use CoT steps paired with tool use to solve very complex problems, this is what we see in OpenAI's current flagship model `o1`. We'll see later in the course how we can do this ourselves."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}